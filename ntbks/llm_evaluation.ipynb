{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "- bert-score\n",
    "- evaluate\n",
    "    - rouge-score\n",
    "- deepeval ( This says it's intergrated with Ollama but I wasn't able to make it work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepeval\n",
    "I am using custom LLM to plug in into deepeval for evaluation. I am using llama3.2:latest from ollama. If you are doing the same please run the following commands in your CLI.\n",
    "\n",
    "- deepeval set-ollama llama3.2:latest \n",
    "\n",
    "Once you are done you can use the following command to reset\n",
    "- deepeval unset-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"You have 30 days to get a full refund at no extra cost.\"]\n",
    "references = [\"We offer a 30-day full refund at no extra costs.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Score\n",
    "\n",
    "Paper - https://arxiv.org/pdf/1904.09675\n",
    "\n",
    "High level definition\n",
    "- Extract the contextual embedding for each token in the answer and the reference answer\n",
    "- Consine similarity across each answer word to reference word combination\n",
    "- Precision would pick the max score with reference as the answer from the LLM\n",
    "- Recall would pick the max score with reference as reference answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 118.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.55 seconds, 0.65 sentences/sec\n",
      "Precision: 0.9305, Recall: 0.9501, F1: 0.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "P, R, F1 = score(predictions, references, lang='en', verbose=True)\n",
    "print(f\"Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROGUE score\n",
    "Recall-Oriented Understudy for Gisting Evaluation\n",
    "Paper - https://aclanthology.org/W04-1013.pdf\n",
    "\n",
    "- For unigram, matching words/ numeber of words in the reference sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.5833333333333334), 'rouge2': np.float64(0.3636363636363636), 'rougeL': np.float64(0.4999999999999999), 'rougeLsum': np.float64(0.4999999999999999)}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "rouge = load('rouge')\n",
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing custom evaluation from local ollama model (llama 3.2 latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Is GE Calculated?\n",
    "\n",
    "Since G-Eval is a two-step algorithm that generates chain of thoughts (CoTs) for better evaluation, in deepeval this means first generating a series of evaluation_steps using CoT based on the given criteria, before using the generated steps to determine the final score using the parameters presented in an LLMTestCase.\n",
    "\n",
    "When you provide evaluation_steps, the GEval metric skips the first step and uses the provided steps to determine the final score instead, make it more reliable across different runs. If you don't have a clear evaluation_stepss, what we've found useful is to first write a criteria which can be extremely short, and use the evaluation_steps generated by GEval for subsequent evaluation and fine-tuning of criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "def test_case():\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.5\n",
    "    )\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n",
    "        expected_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "        retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    "    )\n",
    "    assert_test(test_case, [correctness_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
