{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "- bert-score\n",
    "- evaluate\n",
    "    - rouge-score\n",
    "- deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepeval\n",
    "I am using custom LLM to plug in into deepeval for evaluation. I am using llama3.2:latest from ollama. If you are doing the same please run the following commands in your CLI.\n",
    "\n",
    "- deepeval set-ollama llama3.2:latest \n",
    "\n",
    "Once you are done you can use the following command to reset\n",
    "- deepeval unset-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"You have 30 days to get a full refund at no extra cost.\"]\n",
    "references = [\"We offer a 30-day full refund at no extra costs.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 118.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.55 seconds, 0.65 sentences/sec\n",
      "Precision: 0.9305, Recall: 0.9501, F1: 0.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "P, R, F1 = score(predictions, references, lang='en', verbose=True)\n",
    "print(f\"Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.5833333333333334), 'rouge2': np.float64(0.3636363636363636), 'rougeL': np.float64(0.4999999999999999), 'rougeLsum': np.float64(0.4999999999999999)}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "rouge = load('rouge')\n",
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Is GE Calculated?\n",
    "\n",
    "Since G-Eval is a two-step algorithm that generates chain of thoughts (CoTs) for better evaluation, in deepeval this means first generating a series of evaluation_steps using CoT based on the given criteria, before using the generated steps to determine the final score using the parameters presented in an LLMTestCase.\n",
    "\n",
    "When you provide evaluation_steps, the GEval metric skips the first step and uses the provided steps to determine the final score instead, make it more reliable across different runs. If you don't have a clear evaluation_stepss, what we've found useful is to first write a criteria which can be extremely short, and use the evaluation_steps generated by GEval for subsequent evaluation and fine-tuning of criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "def test_case():\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.5\n",
    "    )\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n",
    "        expected_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "        retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    "    )\n",
    "    assert_test(test_case, [correctness_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_case\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtest_case\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_case\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     correctness_metric = \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCorrectness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDetermine if the \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mactual output\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m is correct based on the \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexpected output\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mACTUAL_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEXPECTED_OUTPUT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     test_case = LLMTestCase(\n\u001b[32m     14\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mWhat if these shoes don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt fit?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# Replace this with the actual output from your LLM application\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m         retrieval_context=[\u001b[33m\"\u001b[39m\u001b[33mAll customers are eligible for a 30 day full refund at no extra costs.\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m     assert_test(test_case, [correctness_metric])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/deepeval/metrics/g_eval/g_eval.py:103\u001b[39m, in \u001b[36mGEval.__init__\u001b[39m\u001b[34m(self, name, evaluation_params, criteria, evaluation_steps, model, threshold, top_logprobs, async_mode, strict_mode, verbose_mode, _include_g_eval_suffix)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mevaluation_steps\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must not be an empty list. Either omit evaluation steps or include a non-empty list of steps.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28mself\u001b[39m.criteria = criteria\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.using_native_model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_model = \u001b[38;5;28mself\u001b[39m.model.get_model_name()\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_steps = evaluation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/deepeval/metrics/utils.py:323\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AzureOpenAIModel(model=model), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Otherwise (the model is a wrong type), we raise an error\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    327\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, OllamaModel, LocalModel.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    328\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/deepeval/models/llms/openai_model.py:160\u001b[39m, in \u001b[36mGPTModel.__init__\u001b[39m\u001b[34m(self, model, _openai_api_key, base_url, *args, **kwargs)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.args = args\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/deepeval/models/base_model.py:35\u001b[39m, in \u001b[36mDeepEvalBaseLLM.__init__\u001b[39m\u001b[34m(self, model_name, *args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *args, **kwargs):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/deepeval/models/llms/openai_model.py:347\u001b[39m, in \u001b[36mGPTModel.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_openai_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:622\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    618\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(\n\u001b[32m    619\u001b[39m             proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy, verify=global_ssl_context\n\u001b[32m    620\u001b[39m         )\n\u001b[32m    621\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[38;5;28mself\u001b[39m.root_client.chat.completions\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llm/lib/python3.13/site-packages/openai/_client.py:114\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    112\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "test_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepeval.metrics import AnswerRelevancyMetric\n",
    "# from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "# test_case = LLMTestCase(\n",
    "#     input=\"What if these shoes don't fit?\",\n",
    "#     # Replace this with the actual output from your LLM application\n",
    "#     actual_output=str(predictions,\n",
    "#     retrieval_context=references\n",
    "# )\n",
    "\n",
    "# answer_relevancy_metric.measure(test_case)\n",
    "# print(answer_relevancy_metric.score)\n",
    "# # All metrics also offer an explanation\n",
    "# print(answer_relevancy_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
