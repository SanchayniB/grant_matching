{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_llama = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je aime le programmation.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "output = llm_llama.invoke(messages)\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mistral with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "dotenv_path = Path('/Users/sanchaynibagade/Documents/github/grant_matching/env/mistral.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "MY_KEY = os.getenv('MISTRAL_KEY')\n",
    "os.environ[\"MISTRAL_API_KEY\"] = MY_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_mistral = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The translation of \"I love programming\" into French is:\\n\\n\"J\\'aime la programmation.\"\\n\\nHere\\'s a breakdown:\\n- \"I love\" = \"J\\'aime\"\\n- \"programming\" = \"la programmation\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\",\"You are a helpful assistant that translates English to French. Translate the user sentence.\"),\n",
    "    (\"human\",\"I love programming.\")\n",
    "]\n",
    "output = llm_mistral.invoke(messages)\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The result of 81 divided by 9 is 9.\\n\\nHere's the calculation:\\n\\n81 ÷ 9 = 9\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 12, 'total_tokens': 41, 'completion_tokens': 29}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run-ccb2b0f1-9e03-4757-85a4-dafcb59e170e-0', usage_metadata={'input_tokens': 12, 'output_tokens': 29, 'total_tokens': 41})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_mistral.invoke(\"What is 81 divided by 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing chatbot format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 10 times 5 is 50.\n",
      "\n",
      "Here it is:\n",
      "\n",
      "10\n",
      "x  5\n",
      "----\n",
      "  50\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
    "    HumanMessage(content=\"What is 10 times 5?\")\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = llm_mistral.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 10 × 5 = 50\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with messages\n",
    "result = llm_llama.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 53 times 7 is 371.\n",
      "\n",
      "Here's the multiplication:\n",
      "\n",
      "  53\n",
      "x  7\n",
      "-----\n",
      " 371\n",
      "Answer from AI: 53 × 7 = 371\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
    "    HumanMessage(content=\"What is 10 times 5?\"),\n",
    "    AIMessage(content=\"10 times 5 is 50\"),\n",
    "    HumanMessage(content=\"What is 53 times 7?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = llm_mistral.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = llm_llama.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
